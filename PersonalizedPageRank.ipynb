{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive"
      ],
      "metadata": {
        "id": "UDrt8Vem8TNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gP0HSmRm4vcD",
        "outputId": "5c8a5f25-ecfb-49e6-c590-ad064f9ac1eb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "UPYkzpiv8VZg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2L6noIt6vsGX"
      },
      "outputs": [],
      "source": [
        "!apt-get update -qq > /dev/null\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.4.2/spark-3.4.2-bin-hadoop3.tgz\n",
        "!tar xf spark-3.4.2-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.2-bin-hadoop3\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import random\n",
        "spark = SparkSession.builder.appName(\"YourTest\")\\\n",
        ".master(\"local[2]\")\\\n",
        ".config('spark.ui.port', random.randrange(4000,5000))\\\n",
        ".config(\"spark.jars.packages\",\"graphframes:graphframes:0.8.3-spark3.4-s_2.12\")\\\n",
        ".getOrCreate()"
      ],
      "metadata": {
        "id": "aqThub_mv9n7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "ui_port=spark.sparkContext.uiWebUrl.split(\":\")[-1]\n",
        "output.serve_kernel_port_as_window(ui_port,path='/jobs/index.html')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "pOf94EqyBiG-",
        "outputId": "f574db0f-2732-4217-e651-978507244ac9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(4620, \"/jobs/index.html\", \"https://localhost:4620/jobs/index.html\", window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark.sql.functions import coalesce, col, lit, sum, when\n",
        "import pyspark.sql.functions as f\n",
        "from graphframes import *\n",
        "from graphframes.lib import Pregel\n",
        "\n",
        "spark.sparkContext.setCheckpointDir(\"./checkpoint/\")"
      ],
      "metadata": {
        "id": "FuEGeZ-D9Zm9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Personalized Page Rank"
      ],
      "metadata": {
        "id": "ZAkFJmjNJ1p6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from graphframes.lib import Pregel\n",
        "from pyspark.sql.functions import lit, col, abs as pyspark_abs\n",
        "\n",
        "# Implement in_degree function\n",
        "def in_Degrees(graph):\n",
        "  in_degrees = graph.edges.groupBy(\"dst\").count().select(col(\"dst\").alias(\"id\"), col(\"count\").alias(\"in_degree\"))\n",
        "\n",
        "  # for 0 indegree\n",
        "  return graph.vertices.join(in_degrees, \"id\", \"left_outer\").select(\"id\", \"in_degree\").na.fill(0)\n",
        "\n",
        "def personalized_page_rank(graph, sourceId, resetProbability=0.15, maxIter=None, tol=None):\n",
        "    \"\"\"\n",
        "    Runs the Personalized PageRank algorithm on the graph.\n",
        "    Note: Exactly one of fixed_num_iter or tolerance must be set.\n",
        "\n",
        "    :param graph: The graph to run the algorithm on.\n",
        "    :param sourceId: The source vertex for the personalized PageRank.\n",
        "    :param resetProbability: Probability of resetting to the source vertex.\n",
        "    :param maxIter: If set, the algorithm is run for a fixed number\n",
        "            of iterations. This may not be set if the `tol` parameter is set.\n",
        "    :param tol: If set, the algorithm is run until the given tolerance.\n",
        "            This may not be set if the `numIter` parameter is set.\n",
        "    :return:  GraphFrame with new vertices column \"pagerank\" and new edges column \"weight\"\n",
        "    \"\"\"\n",
        "\n",
        "    num_vertex = graph.vertices.count()\n",
        "\n",
        "    # init rank\n",
        "    ranks = graph.vertices.withColumn('pagerank', f.when(graph.vertices.id == sourceId, f.lit(1.0)).otherwise(f.lit(0.0)))\n",
        "\n",
        "    graph_indegrees = in_Degrees(graph)\n",
        "\n",
        "    i = 0\n",
        "    while 1:\n",
        "        i += 1\n",
        "        # calculate contributions for each edges\n",
        "        contributions = graph.edges.join(graph_indegrees, graph.edges.src == graph_indegrees.id).join(ranks,\n",
        "                                                                                                      ranks.id == graph_indegrees.id).select(\n",
        "            col(\"src\"), col(\"dst\"), (col(\"pagerank\") / col(\"in_degree\")).alias(\"contributions\"))\n",
        "\n",
        "        # new pagerank\n",
        "        new = contributions.select(col(\"dst\").alias(\"id\"), col(\"contributions\").alias(\"pagerank\")).groupby(\"id\").sum(\n",
        "            \"pagerank\").withColumnRenamed(\"sum(pagerank)\", \"pagerank\").select(col(\"id\"), (\n",
        "                    resetProbability * f.when(col(\"id\") == sourceId, f.lit(1.0)).otherwise(f.lit(0.0)) + (1 - resetProbability) * col(\"pagerank\")).alias(\"pagerank\")).orderBy(\"id\")\n",
        "\n",
        "        # reach maximum iterations\n",
        "        if maxIter != None and i == maxIter:\n",
        "            return GraphFrame(new, graph.edges)\n",
        "\n",
        "        # difference is smaller than tol\n",
        "        if tol != None:\n",
        "            diff = new.join(ranks.withColumnRenamed('pagerank', 'previous_pagerank'), \"id\")\n",
        "            diff = diff.withColumn('difference', pyspark_abs(diff['previous_pagerank'] - diff['pagerank']))\n",
        "\n",
        "            smallest_difference = diff.orderBy(diff['difference'].desc()).select(\"difference\").limit(1).first()[0]\n",
        "\n",
        "            if smallest_difference < tol:\n",
        "                return GraphFrame(new, graph.edges)\n",
        "        ranks = new\n"
      ],
      "metadata": {
        "id": "oxsSiu_lJASf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Graph Test"
      ],
      "metadata": {
        "id": "py-YknUz8Mat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "edges = spark.createDataFrame([\n",
        "    [0, 1],\n",
        "    [1, 2],\n",
        "    [2, 4],\n",
        "    [2, 0],\n",
        "    [3, 4],\n",
        "    [4, 0],\n",
        "    [4, 2],\n",
        "    [0, 4]\n",
        "    ], [\"src\", \"dst\"])\n",
        "\n",
        "edges.cache()\n",
        "vertices = spark.createDataFrame([[0], [1], [2], [3], [4]], [\"id\"])\n",
        "vertices.show()\n",
        "\n",
        "vertices.cache()\n",
        "graph = GraphFrame(vertices, edges)\n",
        "\n",
        "test = personalized_page_rank(graph, 0, maxIter = 1)\n",
        "test.vertices.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwlU0I175Hju",
        "outputId": "b604d6c1-02e0-4a41-fac2-32f3487bc295"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  0|\n",
            "|  1|\n",
            "|  2|\n",
            "|  3|\n",
            "|  4|\n",
            "+---+\n",
            "\n",
            "+---+--------+\n",
            "| id|pagerank|\n",
            "+---+--------+\n",
            "|  0|    0.15|\n",
            "|  1|   0.425|\n",
            "|  2|     0.0|\n",
            "|  4|   0.425|\n",
            "+---+--------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}