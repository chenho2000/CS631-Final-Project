{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Mount Google Drive**"
   ],
   "metadata": {
    "id": "XmcaQJUOSDr1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZFK8TmGqrw-g",
    "outputId": "50ccc2c5-1d29-4664-d173-39541680bde6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Setup**"
   ],
   "metadata": {
    "id": "Q_BHkJ8BSGxA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!apt-get update -qq > /dev/null\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q https://dlcdn.apache.org/spark/spark-3.4.2/spark-3.4.2-bin-hadoop3.tgz\n",
    "!tar xf spark-3.4.2-bin-hadoop3.tgz\n",
    "!pip install -q findspark"
   ],
   "metadata": {
    "id": "tvFED4yVgjg0"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.2-bin-hadoop3\"\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import random\n",
    "\n",
    "spark = SparkSession.builder.appName(\"YourTest\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .config('spark.ui.port', random.randrange(4000, 5000)) \\\n",
    "    .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.3-spark3.4-s_2.12\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "from graphframes import GraphFrame\n",
    "from pyspark.sql.functions import lit, col, abs as pyspark_abs, when"
   ],
   "metadata": {
    "id": "mCC6uOoEgoNz"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Read the excel file / rename columns\n",
    "twitch_gamers = spark.read.format(\"csv\").option(\"header\", \"true\").load(\n",
    "    \"/content/drive/MyDrive/twitch_gamers/large_twitch_edges_test.csv\").withColumnRenamed(\"numeric_id_1\",\n",
    "                                                                                          \"src\").withColumnRenamed(\n",
    "    \"numeric_id_2\", \"dst\")\n",
    "\n",
    "vertices = twitch_gamers.select(\"src\").union(twitch_gamers.select(\"dst\")).distinct().withColumnRenamed(\"src\", \"id\")\n",
    "\n",
    "edges = twitch_gamers.select(\"src\", \"dst\")\n",
    "\n",
    "graph = GraphFrame(vertices, edges)\n",
    "\n",
    "# Show graph\n",
    "graph.vertices.show()\n",
    "graph.edges.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "26McF-LApnQY",
    "outputId": "9f29c578-a1dd-4626-eb98-fdd67f837c1a"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  3|\n",
      "|  1|\n",
      "|  4|\n",
      "|  2|\n",
      "+---+\n",
      "\n",
      "+---+---+\n",
      "|src|dst|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "|  1|  3|\n",
      "|  1|  4|\n",
      "|  2|  1|\n",
      "|  3|  1|\n",
      "|  4|  1|\n",
      "+---+---+\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**In degree function**"
   ],
   "metadata": {
    "id": "FhuklZVpSQ2r"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Implement in_degree function\n",
    "def in_Degrees(graph):\n",
    "    in_degrees = graph.edges.groupBy(\"dst\").count().select(col(\"dst\").alias(\"id\"), col(\"count\").alias(\"in_degree\"))\n",
    "\n",
    "    # for 0 indegree\n",
    "    return graph.vertices.join(in_degrees, \"id\", \"left_outer\").select(\"id\", \"in_degree\").na.fill(0)"
   ],
   "metadata": {
    "id": "b2D1KSI2WNNq"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "in_Degrees(graph).show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LVcv0vE-ZGqn",
    "outputId": "7655da57-ad5a-4083-b924-4b210c6bff52"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+---------+\n",
      "| id|in_degree|\n",
      "+---+---------+\n",
      "|  3|        1|\n",
      "|  1|        3|\n",
      "|  4|        1|\n",
      "|  2|        1|\n",
      "+---+---------+\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**PageRank**"
   ],
   "metadata": {
    "id": "_yTJ3cQfvwLG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Implement Page Rank\n",
    "\n",
    "def page_rank(graph, resetProbability=0.15, sourceId=None, maxIter=None, tol=None):\n",
    "    \"\"\"\n",
    "    Runs the PageRank algorithm on the graph.\n",
    "    Note: Exactly one of fixed_num_iter or tolerance must be set.\n",
    "\n",
    "    See Scala documentation for more details.\n",
    "\n",
    "    :param resetProbability: Probability of resetting to a random vertex.\n",
    "    :param sourceId: (optional) the source vertex for a personalized PageRank.\n",
    "    :param maxIter: If set, the algorithm is run for a fixed number\n",
    "            of iterations. This may not be set if the `tol` parameter is set.\n",
    "    :param tol: If set, the algorithm is run until the given tolerance.\n",
    "            This may not be set if the `numIter` parameter is set.\n",
    "    :return:  GraphFrame with new vertices column \"pagerank\" and new edges column \"weight\"\n",
    "    \"\"\"\n",
    "\n",
    "    num_vertex = graph.vertices.count()\n",
    "\n",
    "    # init rank\n",
    "    if sourceId is not None:\n",
    "        ranks = graph.vertices.withColumn('pagerank', when(graph.vertices.id == sourceId, lit(1)).otherwise(lit(0)))\n",
    "    else:\n",
    "        ranks = graph.vertices.withColumn('pagerank', lit(1 / num_vertex))\n",
    "\n",
    "    graph_indegrees = in_Degrees(graph)\n",
    "\n",
    "    i = 0\n",
    "    while 1:\n",
    "        i += 1\n",
    "        # calculate contributions for each edges\n",
    "        contributions = graph.edges.join(graph_indegrees, graph.edges.src == graph_indegrees.id).join(ranks,\n",
    "                                                                                                      ranks.id == graph_indegrees.id).select(\n",
    "            col(\"src\"), col(\"dst\"), (col(\"pagerank\") / col(\"in_degree\")).alias(\"contributions\"))\n",
    "\n",
    "        # new pagerank\n",
    "        if sourceId is not None:\n",
    "            new = contributions.select(col(\"dst\").alias(\"id\"), col(\"contributions\").alias(\"pagerank\")).groupby(\n",
    "                \"id\").sum(\n",
    "                \"pagerank\").withColumnRenamed(\"sum(pagerank)\", \"pagerank\").select(col(\"id\"), (\n",
    "                    resetProbability * when(col(\"id\") == sourceId, lit(1)).otherwise(lit(0)) + (\n",
    "                    1 - resetProbability) * col(\"pagerank\")).alias(\"pagerank\")).orderBy(\"id\")\n",
    "        else:\n",
    "            new = contributions.select(col(\"dst\").alias(\"id\"), col(\"contributions\").alias(\"pagerank\")).groupby(\n",
    "                \"id\").sum(\n",
    "                \"pagerank\").withColumnRenamed(\"sum(pagerank)\", \"pagerank\").select(col(\"id\"), (\n",
    "                    resetProbability + (1 - resetProbability) * col(\"pagerank\")).alias(\"pagerank\")).orderBy(\"id\")\n",
    "\n",
    "        # reach maximum iterations\n",
    "        if maxIter is not None and i == maxIter:\n",
    "            return GraphFrame(new, graph.edges)\n",
    "\n",
    "        # difference is smaller than tol\n",
    "        if tol is not None:\n",
    "            diff = new.join(ranks.withColumnRenamed('pagerank', 'previous_pagerank'), \"id\")\n",
    "            diff = diff.withColumn('difference', pyspark_abs(diff['previous_pagerank'] - diff['pagerank']))\n",
    "\n",
    "            smallest_difference = diff.orderBy(diff['difference'].desc()).select(\"difference\").limit(1).first()[0]\n",
    "\n",
    "            if smallest_difference < tol:\n",
    "                return GraphFrame(new, graph.edges)\n",
    "        ranks = new\n",
    "\n"
   ],
   "metadata": {
    "id": "N00HJajGr7u5"
   },
   "execution_count": 24,
   "outputs": []
  }
 ]
}
